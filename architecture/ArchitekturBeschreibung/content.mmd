Base Header Level:  3
latex input:        document-info
latex footer:       x3-paper-end

# Architekturbeschreibung

Die Architektur des Prototype besteht aus drei Hauptkomponenten: Data Extraction, Data Analysis und Visualization, welche außerdem der internen Aufteilung der Gruppe entsprechen. Diese Aufteilung ermöglicht den Teams die Komponenten der jeweils anderen Teams als Blackbox zu betrachten, sodass eine Abstimmung lediglich über die Funktionalität (was wird umgesetzt), nicht aber über die konkrete Implementation (wie wird es umgesetzt) der Komponenten erfolgen muss.

Im folgenden wird zuerst die Schnittstelle zwischen den Komponenten, und anschließend jede Komponente einzeln beschrieben.


## Komponentenschnittstelle

Für den Prototyp des Systems wird als zentrale Schnittstelle zwischen den Komponenten eine einfach relationale Datenbank genutzt (MySQL). Diese Designentscheidung wurde getroffen, um eine möglichst einfache Integration der Komponenten zu ermöglichen, und somit den Fokus auf die für den Prototypen relevanteren Themen legen zu können, deren Komplexität innerhalb der Komponenten liegen. Aus der Verwendung einer Datenbank als Schnittstelle ergibt sich der Nachteil, dass die Analyse- sowie die Visualisierungskomponente die Datenbank konstant nach neuen Daten Fragen muss (polling). Als Ergänzung zur Datenbank könnte man hier eine Message Queue einsetzen.



## Data Extraction

Die Data Extraction Komponente bildet die Schnittstelle zwischen den Sozialen Netzen welche wir als Datenquellen benutzen, und unserem System. Für die einzelnen Netzwerke wird hier ein entsprechender Adapter verwendet, welcher generische Daten extrahiert, und außerdem Netzwerk-spezifische Daten (bei Twitter bspw. Retweets) gesondert ablegt, damit diese für die Analyse nicht verloren gehen. Die Adapter persistieren die Daten über das `IPersistenceManager` Interface des `PersistenceManager`s.



## Data Analysis

In der Data Analysis Komponente werden die extrahierten Daten zu Informationen konsolidiert. Hier gibt es ebenfalls für jedes angebundene soziale Netz eine `Analyser` Komponente, welche die Daten in ein für den `InformationExtractor` nutzbares Format bringen, welche er über die `IDataSource` Schnittstelle abfragt. Der `InformationExtractor` verwendet anschließend weitere Analyse-interne Komponenten um Informationen aus den Daten zu gewinnen. Für den Prototyp sind hier ein `KeywordFinder` angedacht, welcher mögliche Schlüsselworte identifiziert, sowie ein `Categorizer`, welcher die Daten in Kategorien einteilt (bspw. Hilfegesuch, Pressemitteilung, ...).


<!--\newpage-->
## Visualization

Die Visualization Komponente ist dafür zuständig die von der Analyse gewonnen Informationen verständlich und interaktiv darzustellen. Ziel ist es, den in den Katastrophenschutz involvierten Behörden einen Überblick über die Situation zu verschaffen, wie sie sich in den sozialen Netzen wiederspiegelt.

Um möglichst einfachen Zugriff auf die Oberfläche zu ermöglichen läuft diese als Webanwendung, womit sie auch über Smartphones und Tablets aufgerufen werden kann. Entsprechend gibt es zwei Komponenten, jene die im Browser ausgeführt wird (Dashboard Client) und jene die auf dem Server ausgeführt wird (Dashboard Server).

<!--\figl{architecture}{UML Komponentensicht auf die Architektur}-->



<!--\newpage-->
# Datenmodell

Das vorliegende Modell beschreibt die aus unserer Sicht vorliegenden fachlichen Daten. Diese sind grob in drei Kategorien zu unterteilen: Rohdaten, verarbeitete Daten, sowie applikationsspezifische Daten. Da das Ziel zunächst ein Prototyp ist fließt dieses in die Entwicklung des Modells mit ein.

Die Rohdaten sind die in der Datenextraktion gesammelten Daten aus den Social Media, sowie potenziell weitere Informationsquellen (z.B. Pegelstände, Trenderkennung etc.). Die Gesamthierarchie der Daten setzt sich daher auf verschiedenen Ebenen zusammen: Grundsätzlich besitzen Daten einen eindeutigen Identifikator und eine Iteration, welche es uns ermöglicht die Datensätze grob nach Erfassungsdatum zu selektieren. Daten aus den Sozialen Medien besitzen im Modell zusätzlich einen Autor, dieser ist nur ein Text welcher einen Namen darstellt (diese Einschränkung ist nötig um nicht direkt personenbezogene Daten zu sammeln). Des Weiteren besitzt jeder Datensatz der sozialen Medien ein spezifisches Veröffentlichungsdatum.

Da sich unser Prototyp zunächst auf Twitter als Plattform bezieht nutzen wir im ersten Schritt nur Tweets als Informationsquelle. Diese enthalten im Datenmodell den eigentlichen Inhalt, die Anzahl der aktuellen Retweets und Follower (des Nutzers), sowie die Geolocation und einen vom Absender definierten Wohnort, sowie eine Referenz auf einen möglichen ursprünglichen Tweet. Diese Angaben sind allerdings nur dann vorhanden wenn sie vom Tweetverfasser zur Verfügung gestellt wurden. Darüber hinaus verfügt jeder Tweet über beliebig viele Hashtags, welche ihm zugeordnet sind.

Die verarbeiteten Daten sind zunächst eine reine Kopie der Rohdaten und werden durch die Analysegruppe durch gewonnene Informationen erweitert. Für das erste Anwendungsszenario soll daher zunächst jedem Tweet eine Kategorie zugewiesen werden. Diese soll grob den Inhalt des Tweets wiedergeben. Weitere Informationen sind ebenfalls geplant. Die dritte Kategorie, welche im Modell genannt wird gehört zur eigentlichen Applikation. Diese enthält ein Dashboard mit Informationen welche in beliebig vielen Widgets dargestellt werden. Darüber hinaus sind für die Applikation Nutzer samt eines rudimentären Authentifikationsverfahrens anhand einer E-Mail Adresse und eines Passworts vorgesehen.

Für den bisherigen Prototypen fallen alle genannten Informationen auf einer Ebene zusammen, sodass die Datenbanktabelle dementsprechend gestaltet ist.

<!--\figl{datamodel}{Fachliches Datenmodell}-->
